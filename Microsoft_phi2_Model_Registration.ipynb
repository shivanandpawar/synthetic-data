{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ea0560",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b771c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze | grep transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f1cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall urllib3 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeaae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install urllib3==1.26.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5d7b5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "/tmp/pip_packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "######################################################################################################\n",
    "# Model Registration\n",
    "\n",
    "from refractml import *\n",
    "from refractml.constants import MLModelFlavours\n",
    "\n",
    "# new score functions\n",
    "from mosaic_utils.ai.score.base import ScoreBase\n",
    "from typing import Tuple, Union, List, Any\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "class ScoreTemplateExample(ScoreBase):\n",
    "    \"\"\"\n",
    "    This Class Demonstrate How To Implements ScoreBase Interface Class And It Basic Usage.\n",
    "    \"\"\"    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        import os\n",
    "        os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "        import torch\n",
    "        from torch import cuda\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "#        self.token_path = '/data/models/mistralai_base/token.pkl'\n",
    "#        self.model_path = '/data/models/mistralai_base/qa.pkl'\n",
    "        if self.model is None:\n",
    "            print(\"Load Model From Data Volume\")\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\"/data/huggingface/cache/models--microsoft--phi-2/snapshots/b10c3eba545ad279e7208ee3a5d644566f001670\",device_map = 'cpu',torch_dtype=torch.bfloat16,trust_remote_code=True)\n",
    "#             self.model_loaded =torch.load(self.model_path)\n",
    "#            self.model.generate(bos_token_id=tokenizer.bos_token_id)\n",
    "#            print(\"model loaded \")\n",
    "\n",
    "        if self.tokenizer is None:\n",
    "            print(\"tokenizer object is loading from data section\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"/data/huggingface/cache/models--microsoft--phi-2/snapshots/b10c3eba545ad279e7208ee3a5d644566f001670\",trust_remote_code=True)\n",
    "#             self.tokenizer = torch.load(self.token_path)\n",
    "            print(\"tokenizer loaded\")\n",
    "\n",
    "        self.model.generate(bos_token_id=self.tokenizer.bos_token_id)\n",
    "        print(\"model loaded \")\n",
    "#        !sudo chmod 777 /home/mosaic-ai/.cache/huggingface\n",
    "    \n",
    "    def request_processing_fn(self, request) :\n",
    "        \"\"\"\n",
    "        Processes Request Object -> List[Input data, ..]. It could be:\n",
    "               A List Mapping of All Value Can Be one of : \n",
    "                   - List[ [Feature_Value1, Feature_Value2, ...], [...] ]\n",
    "                   - List[numpy.array(), numpy.array(), ...]\n",
    "                   - List[tf.Tensor, tf.Tensor, tf.Tensor, ...]\n",
    "                   - List[ SingleSample, SingleSample]\n",
    "                   \n",
    "        :return: (n_inputs, payload's)\n",
    "        \n",
    "        Warnings:\n",
    "        1. Do not reshape your final output for single sample here, do it in prediction.\n",
    "           Else payloads will be invalidated for extraction at raw and extraction level.\n",
    "        \"\"\"\n",
    "        final_payload = []\n",
    "        raw_payload = request.json[\"payload\"]\n",
    "        return (1, raw_payload) \n",
    "    \n",
    "    def pre_processing_fn(self,payload):\n",
    "        # All preprocessing step must occur in this section\n",
    "        # Takes Single Sample -> Returns Single Sample\n",
    "        \n",
    "        # Not Doing Any Preprocessing Hence Returned payload\n",
    "        print(\"payload is \", payload)\n",
    "        \n",
    "        return payload\n",
    "\n",
    "    def prediction_fn(self,\n",
    "                      model: Any,\n",
    "                      pre_processed_input \n",
    "                      ):\n",
    "        \"\"\"\n",
    "                Does the main prediction on pre_processed_input(Single Sample) using supplied model .\n",
    "\n",
    "                :param model: Supported Model\n",
    "                :param pre_processed_input: Single Preprocessed Payload\n",
    "                :return: Prediction Value From the model\n",
    "                \n",
    "                Important Notes:\n",
    "                - Reshape your data array.reshape(1, -1) before predictions as it contains a single sample.\n",
    "                    \n",
    "        \"\"\"\n",
    "#         device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "#         print(f'Device: ------------- {device}')\n",
    "        input_data = pre_processed_input\n",
    "        print(type(input_data))\n",
    "        print(input_data)\n",
    "        prompt = input_data\n",
    "        input_ids=self.tokenizer.encode(prompt,return_tensors='pt')\n",
    "        output_ids=self.model.generate(input_ids,max_length=500,temperature=0.6,bos_token_id=self.tokenizer.bos_token_id,pad_token_id=self.tokenizer.pad_token_id)\n",
    "        generated_text=self.tokenizer.decode(output_ids[0],skip_special_tokens=True)\n",
    "        #print(generated_text)\n",
    "        #print(sequences[0]['generated_text'])\n",
    "        #print(\"inference generated\")\n",
    "        return generated_text\n",
    "\n",
    "    class Meta:    \n",
    "        # List of Callables() can be attached For Calling After AnSd Before Scoring\n",
    "        def __init__(self):\n",
    "            self.name = \"Pre Hooked Me !\"\n",
    "            self.pre_call_hooks.append(self.print_)\n",
    "        def print_(self):\n",
    "            print(self.name)\n",
    "        pre_call_hooks = []\n",
    "        post_call_hooks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74d1db08",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"How to reverse a string in python?\"\n",
    "import requests\n",
    "req = requests.Request()\n",
    "req.json = {\"payload\":prompt\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd1d8d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Model From Data Volume\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eca43a458fd4ca6867290bcea4d52be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer object is loading from data section\n",
      "tokenizer loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/pip_packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded \n"
     ]
    }
   ],
   "source": [
    "score_ = ScoreTemplateExample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f980271b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's date: 2024-03-09 12:57:37.662127\n",
      "1709989057.662275\n",
      "Pre Hooked Me !\n",
      "Pre Hooked Me !\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VALIDATION</th>\n",
       "      <th>COMPONENT</th>\n",
       "      <th>PASSED</th>\n",
       "      <th>SKIPPED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Return Type Must Be Tuple (n_input, payloads)</td>\n",
       "      <td>request_processing_fn</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tuple Must Be of length Two (n_input, payloads)</td>\n",
       "      <td>request_processing_fn</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>* if n_input &gt; 1 payload type must be List (n_input, [np.ndarray, tf.Tensor, etc])</td>\n",
       "      <td>request_processing_fn</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           VALIDATION  \\\n",
       "0                                       Return Type Must Be Tuple (n_input, payloads)   \n",
       "1                                     Tuple Must Be of length Two (n_input, payloads)   \n",
       "2  * if n_input > 1 payload type must be List (n_input, [np.ndarray, tf.Tensor, etc])   \n",
       "\n",
       "               COMPONENT  PASSED  SKIPPED  \n",
       "0  request_processing_fn    True    False  \n",
       "1  request_processing_fn    True    False  \n",
       "2  request_processing_fn    True    False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/pip_packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields Marked Asterisk (*) Can Be Validated On Proper Input \n",
      "\n",
      "payload is  How to reverse a string in python?\n",
      "<class 'str'>\n",
      "How to reverse a string in python?\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "print(\"Today's date:\", datetime.now())\n",
    "t1 = time.time()\n",
    "print(t1)\n",
    "model_predictions = score_.score(None, req, dry_run=True)\n",
    "print(\"Microsoft-phi2 Testing\")\n",
    "t2 = time.time()\n",
    "t2-t1\n",
    "print(\"Today's date:\", datetime.now())\n",
    "print(\"ended the pipe\")\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec5a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(model_predictions))\n",
    "print(len(model_predictions))\n",
    "print((model_predictions))\n",
    "print((model_predictions[0]))\n",
    "print(type(model_predictions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84f4b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_model(None,\n",
    "               ScoreTemplateExample,\n",
    "               \"Microsoft_phi2_Testing\",\n",
    "               \"Microsoft_phi2\",\n",
    "               MLModelFlavours.pytorch,\n",
    "               init_script=\"pip install SentencePiece \\\\n pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\"\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773645ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
